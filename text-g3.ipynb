{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 1: Data Loading and Preprocessing\n",
    "بخش 1: بارگذاری داده‌ها و پیش‌پردازش\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string, os \n",
    "\n",
    "# Setting the current directory\n",
    "curr_dir = './'\n",
    "\n",
    "# Initializing a list to store all headlines\n",
    "all_headlines = []\n",
    "\n",
    "# Loading data from CSV files\n",
    "for filename in os.listdir(curr_dir):\n",
    "    if 'Articles' in filename:\n",
    "        # Reading the CSV file\n",
    "        article_df = pd.read_csv(curr_dir + filename)\n",
    "        \n",
    "        # Extending the list with headline values from the DataFrame\n",
    "        all_headlines.extend(list(article_df.headline.values))\n",
    "        \n",
    "        # Breaking the loop after reading the first matching file\n",
    "        break\n",
    "\n",
    "# Removing the \"Unknown\" expression from headlines\n",
    "all_headlines = [h for h in all_headlines if h != \"Unknown\"]\n",
    "\n",
    "# Text cleaning function\n",
    "def clean_text(txt):\n",
    "    \"\"\"\n",
    "    Cleans the given text by removing punctuation and converting to lowercase.\n",
    "\n",
    "    Args:\n",
    "    - txt (str): Input text.\n",
    "\n",
    "    Returns:\n",
    "    - txt (str): Cleaned text.\n",
    "    \"\"\"\n",
    "    # Removing punctuation and converting to lowercase\n",
    "    txt = \"\".join(v for v in txt if v not in string.punctuation).lower()\n",
    "    txt = txt.encode(\"utf8\").decode(\"ascii\",'ignore')\n",
    "    return txt \n",
    "\n",
    "# Creating cleaned text\n",
    "corpus = [clean_text(x) for x in all_headlines]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This part of the code includes commands for loading data from CSV files, removing the \"Unknown\" expression from headlines, and a text cleaning function. The cleaned text is then created based on the headlines.\n",
    "\n",
    "\n",
    "در این بخش از کد، دستورات لازم برای بارگذاری داده‌ها از فایل CSV، حذف عبارت \"Unknown\" از عناوین، و تابع پاک‌سازی متن به همراه ایجاد متن پاک‌سازی شده انجام میشه."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 2: Tokenization and Data to Tokens Conversion\n",
    "بخش 2: توکن‌سازی و تبدیل داده به توکن‌ها"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing necessary libraries\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "# Creating a tokenizer object\n",
    "tokenizer = Tokenizer()\n",
    "\n",
    "# Tokenization function\n",
    "def get_sequence_of_tokens(corpus):\n",
    "    \"\"\"\n",
    "    Tokenizes the given corpus and generates input sequences for training.\n",
    "\n",
    "    Args:\n",
    "    - corpus (list): A list of text data.\n",
    "\n",
    "    Returns:\n",
    "    - input_sequences (list): List of input sequences for training.\n",
    "    - total_words (int): Total number of unique words in the corpus.\n",
    "    \"\"\"\n",
    "    # Fitting the tokenizer on the corpus\n",
    "    tokenizer.fit_on_texts(corpus)\n",
    "    \n",
    "    # Calculating the total number of unique words\n",
    "    total_words = len(tokenizer.word_index) + 1\n",
    "\n",
    "    # Generating input sequences\n",
    "    input_sequences = []\n",
    "    for line in corpus:\n",
    "        token_list = tokenizer.texts_to_sequences([line])[0]\n",
    "        for i in range(1, len(token_list)):\n",
    "            n_gram_sequence = token_list[:i+1]\n",
    "            input_sequences.append(n_gram_sequence)\n",
    "    \n",
    "    return input_sequences, total_words\n",
    "\n",
    "# Generating token sequences and getting total number of words\n",
    "inp_sequences, total_words = get_sequence_of_tokens(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "In this part of the code, using the Keras Tokenizer library, tokenization is performed. In this process, the input text is converted into numerical tokens. The tokenizer, in addition to fitting on the corpus, calculates the total number of unique words in the text. It then generates input sequences for training the LSTM model.\n",
    "\n",
    "\n",
    "این قسمت از کد با استفاده از کتابخانه Keras Tokenizer یک توکن‌سازی رو انجام میده. در این توکن‌سازی، متن ورودی به توکن‌های عددی تبدیل میشه. توکن‌سازی در کنار تعداد کل کلمات یکتا در متن رو محاسبه میکنه و سپس دنباله توکن‌های ورودی برای آموزش مدل LSTM تولید میکنه."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 3: Generating Padded Input Sequences\n",
    "بخش 3: تولید داده‌های ورودی با پدینگ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing necessary libraries\n",
    "from keras.utils import pad_sequences\n",
    "import keras.utils as ku\n",
    "\n",
    "# Function for generating padded input sequences\n",
    "def generate_padded_sequences(input_sequences):\n",
    "    \"\"\"\n",
    "    Generates padded input sequences for training the LSTM model.\n",
    "\n",
    "    Args:\n",
    "    - input_sequences (list): List of input sequences.\n",
    "\n",
    "    Returns:\n",
    "    - predictors (array): Padded input sequences excluding the last element.\n",
    "    - label (array): Last element of each input sequence as one-hot encoded labels.\n",
    "    - max_sequence_len (int): Maximum length of the input sequences after padding.\n",
    "    \"\"\"\n",
    "    # Finding the maximum sequence length\n",
    "    max_sequence_len = max([len(x) for x in input_sequences])\n",
    "    \n",
    "    # Padding input sequences\n",
    "    input_sequences = np.array(pad_sequences(input_sequences, maxlen=max_sequence_len, padding='pre'))\n",
    "    \n",
    "    # Extracting predictors and labels\n",
    "    predictors, label = input_sequences[:,:-1], input_sequences[:,-1]\n",
    "    \n",
    "    # One-hot encoding labels\n",
    "    label = ku.to_categorical(label, num_classes=total_words)\n",
    "    \n",
    "    return predictors, label, max_sequence_len\n",
    "\n",
    "# Generating padded input sequences\n",
    "predictors, label, max_sequence_len = generate_padded_sequences(inp_sequences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part of the code, the Keras library is used for padding input data. The function generate_padded_sequences produces padded input sequences for training the LSTM model. This function includes the matrix predictors (input sequences excluding the last element), the matrix label (the last element of each sequence as one-hot encoded labels), and max_sequence_len (the maximum length of input sequences after padding).\n",
    "\n",
    "\n",
    "در این بخش از کد، از کتابخانه Keras برای پدینگ داده‌های ورودی استفاده شده. تابع generate_padded_sequences دنباله‌های ورودی رو با پدینگ تولید میکنه. این تابع شامل ماتریس predictors که دنباله‌های ورودی هستند (با حذف آخرین عنصر)، ماتریس label که آخرین عنصر هر دنباله به صورت one-hot encoded به عنوان برچسب‌هاست، و max_sequence_len که طول بیشترین دنباله پس از پدینگه هست."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 4: Model Creation\n",
    "بخش 4: ایجاد مدل"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing necessary libraries\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, LSTM, Dense, Dropout\n",
    "\n",
    "# Function for creating the model\n",
    "def create_model(max_sequence_len, total_words):\n",
    "    \"\"\"\n",
    "    Creates and compiles an LSTM model for text generation.\n",
    "\n",
    "    Args:\n",
    "    - max_sequence_len (int): Maximum length of the input sequences after padding.\n",
    "    - total_words (int): Total number of unique words in the corpus.\n",
    "\n",
    "    Returns:\n",
    "    - model (Sequential): Compiled LSTM model for text generation.\n",
    "    \"\"\"\n",
    "    # Setting the input length\n",
    "    input_len = max_sequence_len - 1\n",
    "    \n",
    "    # Initializing a Sequential model\n",
    "    model = Sequential()\n",
    "    \n",
    "    # Adding an Embedding Layer\n",
    "    model.add(Embedding(total_words, 10, input_length=input_len))\n",
    "    \n",
    "    # Adding Hidden Layer 1 - LSTM\n",
    "    model.add(LSTM(100))\n",
    "    model.add(Dropout(0.1))\n",
    "    \n",
    "    # Adding Output Layer\n",
    "    model.add(Dense(total_words, activation='softmax'))\n",
    "\n",
    "    # Compiling the model\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Creating and compiling the LSTM model\n",
    "model = create_model(max_sequence_len, total_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part of the code, an LSTM model for text generation is created and compiled. The create_model function includes a Sequential model with an Embedding layer, an LSTM layer as the hidden layer, and a Dense layer as the output layer. The model is compiled using categorical crossentropy as the loss function and the Adam optimization algorithm.\n",
    "\n",
    "\n",
    "در این بخش از کد، یک مدل LSTM برای تولید متن ایجاد شده و کامپایل میشه. تابع create_model شامل یک مدل Sequential با یک لایه Embedding، یک لایه LSTM به عنوان لایه مخفی، و یک لایه Dense به عنوان لایه خروجیه. مدل با استفاده از الگوریتم categorical crossentropy به عنوان تابع هزینه و الگوریتم بهینه‌سازی Adam کامپایل میشه."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 5: Model Training and Saving Weights\n",
    "\n",
    "بخش 5: آموزش مدل و ذخیره وزن‌ها"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "Epoch 2/100\n",
      "Epoch 3/100\n",
      "Epoch 4/100\n",
      "Epoch 5/100\n",
      "Epoch 6/100\n",
      "Epoch 7/100\n",
      "Epoch 8/100\n",
      "Epoch 9/100\n",
      "Epoch 10/100\n",
      "Epoch 11/100\n",
      "Epoch 12/100\n",
      "Epoch 13/100\n",
      "Epoch 14/100\n",
      "Epoch 15/100\n",
      "Epoch 16/100\n",
      "Epoch 17/100\n",
      "Epoch 18/100\n",
      "Epoch 19/100\n",
      "Epoch 20/100\n",
      "Epoch 21/100\n",
      "Epoch 22/100\n",
      "Epoch 23/100\n",
      "Epoch 24/100\n",
      "Epoch 25/100\n",
      "Epoch 26/100\n",
      "Epoch 27/100\n",
      "Epoch 28/100\n",
      "Epoch 29/100\n",
      "Epoch 30/100\n",
      "Epoch 31/100\n",
      "Epoch 32/100\n",
      "Epoch 33/100\n",
      "Epoch 34/100\n",
      "Epoch 35/100\n",
      "Epoch 36/100\n",
      "Epoch 37/100\n",
      "Epoch 38/100\n",
      "Epoch 39/100\n",
      "Epoch 40/100\n",
      "Epoch 41/100\n",
      "Epoch 42/100\n",
      "Epoch 43/100\n",
      "Epoch 44/100\n",
      "Epoch 45/100\n",
      "Epoch 46/100\n",
      "Epoch 47/100\n",
      "Epoch 48/100\n",
      "Epoch 49/100\n",
      "Epoch 50/100\n",
      "Epoch 51/100\n",
      "Epoch 52/100\n",
      "Epoch 53/100\n",
      "Epoch 54/100\n",
      "Epoch 55/100\n",
      "Epoch 56/100\n",
      "Epoch 57/100\n",
      "Epoch 58/100\n",
      "Epoch 59/100\n",
      "Epoch 60/100\n",
      "Epoch 61/100\n",
      "Epoch 62/100\n",
      "Epoch 63/100\n",
      "Epoch 64/100\n",
      "Epoch 65/100\n",
      "Epoch 66/100\n",
      "Epoch 67/100\n",
      "Epoch 68/100\n",
      "Epoch 69/100\n",
      "Epoch 70/100\n",
      "Epoch 71/100\n",
      "Epoch 72/100\n",
      "Epoch 73/100\n",
      "Epoch 74/100\n",
      "Epoch 75/100\n",
      "Epoch 76/100\n",
      "Epoch 77/100\n",
      "Epoch 78/100\n",
      "Epoch 79/100\n",
      "Epoch 80/100\n",
      "Epoch 81/100\n",
      "Epoch 82/100\n",
      "Epoch 83/100\n",
      "Epoch 84/100\n",
      "Epoch 85/100\n",
      "Epoch 86/100\n",
      "Epoch 87/100\n",
      "Epoch 88/100\n",
      "Epoch 89/100\n",
      "Epoch 90/100\n",
      "Epoch 91/100\n",
      "Epoch 92/100\n",
      "Epoch 93/100\n",
      "Epoch 94/100\n",
      "Epoch 95/100\n",
      "Epoch 96/100\n",
      "Epoch 97/100\n",
      "Epoch 98/100\n",
      "Epoch 99/100\n",
      "Epoch 100/100\n"
     ]
    }
   ],
   "source": [
    "# Importing necessary libraries\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "# Model training\n",
    "model.fit(predictors, label, epochs=100, verbose=5)\n",
    "\n",
    "# Saving model weights\n",
    "model.save_weights('textg3_model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part of the code, the model is trained using the input data (predictors and label) for 100 epochs, and the weights of the model are saved as the 'textg3_model.h5' file. The verbose=5 parameter ensures that more detailed information about the training process is displayed in the console.\n",
    "\n",
    "\n",
    "این بخش از کد شامل دستورات برای آموزش مدل با استفاده از داده‌های ورودی (predictors و label) به مدت 100 دوره (epoch) و ذخیره وزن‌های مدل به عنوان فایل 'textg3_model.h5' باشه. دستور verbose=5 باعث میشه مراحل آموزش با اطلاعات بیشتری در کنسول نمایش داده بشه."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 6: Text Generation Using Trained Model\n",
    "بخش 6: تولید متن با استفاده از مدل آموزش دیده شده"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My Days Of Horror Brings Geopolitical Whiplash Lacks Sip Of Niger\n"
     ]
    }
   ],
   "source": [
    "# Function for generating text\n",
    "def generate_text(seed_text, next_words, model, max_sequence_len):\n",
    "    \"\"\"\n",
    "    Generates new text based on a seed text using the trained LSTM model.\n",
    "\n",
    "    Args:\n",
    "    - seed_text (str): Seed text for text generation.\n",
    "    - next_words (int): Number of words to generate.\n",
    "    - model: Trained LSTM model.\n",
    "    - max_sequence_len (int): Maximum length of the input sequences after padding.\n",
    "\n",
    "    Returns:\n",
    "    - str: Generated text.\n",
    "    \"\"\"\n",
    "    for _ in range(next_words):\n",
    "        # Converting seed text to token list\n",
    "        token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
    "        \n",
    "        # Padding the token list\n",
    "        token_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')\n",
    "        \n",
    "        # Predicting the next word\n",
    "        predict_x = model.predict(token_list, verbose=0) \n",
    "        predicted = np.argmax(predict_x, axis=1)\n",
    "        \n",
    "        # Converting the predicted index to the corresponding word\n",
    "        output_word = \"\"\n",
    "        for word, index in tokenizer.word_index.items():\n",
    "            if index == predicted:\n",
    "                output_word = word\n",
    "                break\n",
    "        \n",
    "        # Appending the predicted word to the seed text\n",
    "        seed_text += \" \" + output_word\n",
    "    \n",
    "    return seed_text.title()\n",
    "\n",
    "# Generating new text\n",
    "print(generate_text(\"my\", 10, model, max_sequence_len))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This part of the code contains a function named generate_text that generates new text based on a seed text using the trained LSTM model. The function takes arguments such as seed_text (initial text), next_words (number of words to generate), model (trained LSTM model), and max_sequence_len (maximum length of the input sequences after padding). The generated text is then displayed as output.\n",
    "\n",
    "\n",
    "این بخش از کد دارای یک تابع به نام generate_text هست که بر اساس یک متن ابتدایی (seed text) و با استفاده از مدل LSTM آموزش دیده شده، متن جدیدی ایجاد میکنه. تابع شامل متغیرهایی مثل seed_text (متن ابتدایی)، next_words (تعداد کلماتی که باید تولید بشن)، model (مدل LSTM آموزش دیده شده) و max_sequence_len (حداکثر طول دنباله ورودی پس از پدینگ) است. متن تولید شده به عنوان خروجی نمایش داده میشه."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
